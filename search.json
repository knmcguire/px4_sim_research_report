[
  {
    "objectID": "docs/index.html#changelog-report",
    "href": "docs/index.html#changelog-report",
    "title": "PX4 Simulation Integration Survey",
    "section": "1.1 Changelog Report",
    "text": "1.1 Changelog Report\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\nDecember 10, 2025\nAll preperation in single chapter (including forum)\n\n\nNovember 28, 2025\nForum to appendix, written recommendations.\n\n\nNovember 26, 2025\nAdding evaluation of results\n\n\nNovember 24, 2025\nAdjusting of figures and tables\n\n\nNovember 20, 2025\nFinal result ROS-aerial group\n\n\nNovember 11, 2025\nPreliminary results PX4 summit\n\n\nOctober 15, 2025\nSend out final survey\n\n\nOctober 14, 2025\nImproved editing pilot survey results\n\n\nOctober 13, 2025\nAdded final survey changes and justifications\n\n\nOctober 9, 2025\nAnalysis pilot survey\n\n\nSeptember 30, 2025\nForum analysis report\n\n\nSeptember 23, 2025\nSend out pilot survey\n\n\nSeptember 7, 2025\nImprovements to pilot survey (Jay)\n\n\nAugust 28, 2025\nStarted draft pilot survey\n\n\nJuly 31, 2025\nProject description formalized"
  },
  {
    "objectID": "docs/index.html#survey-design-and-methodology",
    "href": "docs/index.html#survey-design-and-methodology",
    "title": "PX4 Simulation Integration Survey",
    "section": "2.1 Survey Design and Methodology",
    "text": "2.1 Survey Design and Methodology\n\n2.1.1 Survey Structure\nThe PX4 Simulator Community Survey was designed to gather feedback from users across all aspects of their simulation experience. The survey is set up in six main sections: demographic background, current simulator usage patterns, technical infrastructure, simulator experience evaluation, feature priorities, and strategic recommendations. The survey uses a mixed-methods approach combining quantitative Likert-scale ratings with qualitative open-ended questions.\nThe survey begins by establishing user segments through profession, simulator expertise level, and PX4 experience duration. This segmentation allows analysis of how different user groups (students, researchers, commercial engineers) experience simulation and what improvements they prioritize.\nQuestions about which simulators users employ, their purposes (research, testing, commercial development), vehicle types, and scale (single vs. multi-vehicle) help map the current PX4 simulation landscape. This information provides insight into how the community currently uses simulation tools.\nHardware and infrastructure questions examine potential barriers to adoption. The survey asks whether users have purchased new systems, what operating systems they use, and where they encounter limitations (CPU, GPU, RAM). This data provides information about current hardware requirements and performance constraints.\nThe survey uses detailed Likert-scale assessments across five dimensions: installation experience, user interface, technical performance, feature support, and community resources. This structured approach allows users to provide feedback on specific aspects of their simulation experience. Open-ended questions at the end allow participants to share observations that may not be captured by the structured questions.\nUsers rate importance of potential improvements across technical capabilities, platform compatibility (OS support, ROS integration), usability, and advanced features. This provides a prioritization matrix showing which capabilities matter most to different segments of the community.\nThe survey concludes with open-ended questions soliciting input on the most important improvements needed and offering opportunities for follow-up interviews. This acknowledges that while quantitative data identifies patterns, qualitative discussion often reveals underlying causes and context that ratings alone cannot capture.\nThe pilot version of the survey can be found in the Appendix ( Section 5 )"
  },
  {
    "objectID": "docs/index.html#pilot-survey-analysis",
    "href": "docs/index.html#pilot-survey-analysis",
    "title": "PX4 Simulation Integration Survey",
    "section": "2.2 Pilot Survey Analysis",
    "text": "2.2 Pilot Survey Analysis\nA pilot survey was conducted with 6 PX4 developers to validate the survey instrument before wider distribution. Respondents were primarily professional engineers with advanced to expert-level simulator expertise, and most have 5+ years of PX4 experience. The results of this pilot survey will be used to fine-tune the final survey before it is sent out to the rest of the community.\n\n2.2.1 Simulator Usage and Goals\nAll pilot participants use Gazebo (Classic or Harmonic). About half also use AirSim, FlightGear, or JSBSim. Primary use cases center on algorithm development, pre-flight testing, and quality assurance in CI pipelines. All respondents simulate only single vehicles, which differs from forum analysis (Section 2.3) where multi-vehicle simulation appears as a recurring topic in the broader community. Vehicle types are dominated by quadcopters, followed by fixed-wing aircraft, with VTOL and specialized vehicles showing lower representation. The equipment carried by these quadcopters is predominantly cameras and perception sensors.\n\n\n2.2.2 Technical Infrastructure\nAlmost all pilot participants had purchased new systems or upgraded existing ones specifically for simulation. Ubuntu dominates the operating system distribution with minimal Windows usage. Despite substantial hardware investment, half report no significant limitations.\n\n\n2.2.3 Current Simulator Experience\nRespondents evaluated their most recent simulator (primarily Gazebo variants) across multiple dimensions. Installation ease received mixed feedback. The overall user experience for experienced developers appeared positive, though opinions on documentation and tutorials quality varied considerably. It is unclear whether respondents were referring to PX4 documentation or simulator documentation. Several comments addressed physics realism, particularly in Gazebo itself. While sensor variety was received positively, sensor models and physics simulation received negative feedback from multiple respondents, with some linking this directly to simulator usefulness. Several open-ended responses mentioned PX4 versioning issues with simulators, noting difficulty in determining compatible version combinations. This observation aligns with patterns observed in the forum analysis (found in Section 2.3).\n\n\n2.2.4 Desired Features and Priority\nIn the Desired Features and Priority section, pilot participants rated several items as very important. Software-in-the-loop (SITL) received higher importance ratings than Hardware-in-the-loop (HITL). ROS/ROS2 integration showed clear consensus as essential. Participants also indicated preference for Docker container support.\nRealistic physics models received higher importance ratings than photorealistic rendering, despite most pilot participants using cameras as main sensors. Multi-agent simulation was rated as very important by half of the pilot participants, though they indicated typically simulating only single vehicles. Communication/network simulation received varied responses.\nParticipants showed preference for the ability to create custom sensor models and easily create robot models. GUI features such as drag-and-drop capabilities or beginner-friendly interfaces scored lower on importance ratings."
  },
  {
    "objectID": "docs/index.html#sec-forum",
    "href": "docs/index.html#sec-forum",
    "title": "PX4 Simulation Integration Survey",
    "section": "2.3 Forum Analysis: PX4 Simulation Integration Issues",
    "text": "2.3 Forum Analysis: PX4 Simulation Integration Issues\nThis analysis examines 152 forum posts from the PX4 discussion forum (discuss.px4.io) spanning September 2024 through September 2025 to identify pressing issues in simulation integration. Several representative forum messages are referenced throughout this text, but the full list of forum posts evaluated is available upon request.\nSeveral categories emerged from this investigation, which will be discussed here.\n\n2.3.1 Version Stability and Platform Support\nForum posts indicate that some PX4 updates have introduced breaking changes. Users report that critical parameters for custom spawn positions were removed without alternatives[1], and Hardware-in-the-Loop functionality broke in newer versions[2]. Platform-specific issues appear more frequently for macOS users, especially those on Apple Silicon[3].\n\n\n2.3.2 Core Simulation Reliability\nUsers report problems with simulation accuracy and stability. Some posts describe Extended Kalman Filter position drift exceeding 50 meters during simple hovering[4], particularly when using custom worlds or external sensors. Flight dynamics issues appear in multiple posts: drones arm correctly but fail to take off, or cannot climb beyond minimal altitudes[5], [6]. One post identifies an architectural limitation preventing custom simulators from running slower than real-time[7].\n\n\n2.3.3 Multi-Vehicle Scalability\nFor multi-vehicle simulation, users report startup times exceeding 20 minutes for multiple drones[8]. Configuration complexity appears in multiple posts, including managing unique ports, distinguishing sensor topics, and coordinating spawn positions[9], [10], [11]. Posts also describe communication failures between ground control stations and multiple vehicles[12], [13].\n\n\n2.3.4 Ecosystem Integration\nMultiple posts describe ROS2 bridge reliability issues where sensor data visible in Gazebo fails to reach ROS2 topics[14], [15]. Offboard control through ROS2 shows inconsistent behavior in several posts, including frame convention problems and actuator control failures[16], [17]. Beyond ROS2, posts describe difficulties integrating alternative simulators like Unity, AirSim, and MATLAB/Simulink[18], [19], [20].\n\n\n2.3.5 Custom Model Development\nPosts describe unpredictable flight behavior when modifying physical properties[21], and unclear physical meaning in the motor control interface[22]. Specialized vehicle types (VTOLs, coaxial configurations, fixed-wing aircraft) appear in several posts describing issues, including dangerous behaviors like uncontrolled dives during transitions[23], [24].\n\n\n2.3.6 Sensor and Environment Fidelity\nPosts describe camera streams failing to initialize properly[25], documented features like terrain following not working[26], and environmental effects such as wind failing to manifest despite available commands[27]. Multiple posts report connection failures with ground control stations when using custom worlds[28], with some requiring undocumented workarounds[29].\n\n\n2.3.7 Deployment and Containerization\nPosts report Docker-based deployment issues including connection failures, missing video feeds, and rendering problems[30], [31], [32]."
  },
  {
    "objectID": "docs/index.html#changes-and-additions-for-final-version-survey",
    "href": "docs/index.html#changes-and-additions-for-final-version-survey",
    "title": "PX4 Simulation Integration Survey",
    "section": "2.4 Changes and Additions for Final Version Survey",
    "text": "2.4 Changes and Additions for Final Version Survey\nThe final version of the PX4 Simulator Community Survey incorporates changes based on pilot feedback and forum analysis. The survey was reduced from approximately 35 questions with over 70 individual items to 29 questions with fewer than 40 items for participants to answer.\n\n2.4.1 Survey Length and Structure\nThe most significant change was removing the entire “Current Simulator Experience” section, which contained 30+ Likert-scale items asking respondents to rate installation ease, user experience, performance, and documentation. Pilot respondents mentioned survey length concerns, and these quantitative ratings showed less consensus compared to open-ended feedback. The survey now retains two open-ended questions asking what users like most and their biggest frustration. The Gazebo team will be conducting their own survey on Gazebo simulator issues separately.\n\n\n2.4.2 Audience Expansion\nThe pilot survey participants were primarily experienced, industrial-oriented developers, all simulating only single vehicles. Forum analysis showed issues (installation problems, multi-vehicle challenges, documentation gaps) that appear to affect novice and intermediate users. The final version adds explicit language in the introduction stating the survey is for “anyone who has used or is interested in using simulation with PX4” and adds “Hobbyist / Independent Developer” as a profession option.\n\n\n2.4.3 Version Compatibility and Integration\nForum analysis (Section 2.3) revealed version compatibility as a recurring topic, with posts describing breaking changes between PX4 releases removing critical features like custom spawn positions. Multiple users expressed frustration that “things worked in one configuration, but does not in another.” The final survey adds explicit questions about version compatibility issues and importance.\n\n\n2.4.4 Multi-Vehicle and Advanced Features\nWhile pilot respondents all simulated single vehicles, forum analysis identified multi-vehicle simulation as a recurring topic. The final survey retains questions about the importance of multi-vehicle simulation and networking communication simulation.\n\n\n2.4.5 Strategic Roadmap Priority\nA new section asks respondents to select their single top priority for PX4 simulation development and justify why. This forced-choice question aims to identify what participants consider the most important issue in their workflow regarding PX4 simulation integration.\nThe final survey can be found in the Appendix ( Section 6 )"
  },
  {
    "objectID": "docs/index.html#professions-and-domains",
    "href": "docs/index.html#professions-and-domains",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.1 Professions and Domains",
    "text": "3.1 Professions and Domains\nThe survey revealed that the largest respondent group consisted of “Professional Engineers/Technical Consultants” (54%), followed by “Researchers” (23%) and “Students” (15%) (see Figure 1). By domain, 44% of respondents worked in commercial aerospace or drone businesses, followed by “Academia and Education” (18%) and “Defense/Government” (16%).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: (Left) Pie-chart showing the primary professions of the survey’s participants and (right) pie-chart with the distribution of domains of the survey participants."
  },
  {
    "objectID": "docs/index.html#experience",
    "href": "docs/index.html#experience",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.2 Experience",
    "text": "3.2 Experience\nIn terms of PX4 autopilot experience, the largest group of participants had 1-3 years of experience (33%), followed by those with 1 month to 1 year (20%), and those with over 5 years (17%). Regarding simulator expertise, most respondents rated themselves as intermediate users (39%), followed by advanced users (34%), with novices comprising the third-largest group (17%). Please see Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: (Left) Pie-chart showing the PX4 experience of the participants and (right) pie-chart showing the robot simulation experience."
  },
  {
    "objectID": "docs/index.html#profile-of-personas",
    "href": "docs/index.html#profile-of-personas",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.3 Profile of Personas",
    "text": "3.3 Profile of Personas\nBased on these responses, we identified five distinct user personas depicted in Figure 3: “Senior Engineers (P1),” “Junior Engineers (P2),” “Academic Researchers (P3),” “Academic Students (P4),” and “Independent Hobbyists (P5).” These personas will be used to analyze and interpret the survey results in subsequent sections.\n\n\n\n\n\n\nFigure 3: Pie-chart with the types of personas selected from the profiles.\n\n\n\n\n3.3.1 P1 - Senior Engineers\nHighly experienced professional engineers with advanced to expert-level simulation expertise. These are the power users who deploy PX4 simulators in production environments for commercial products, defense systems, and critical applications.\n\nProfession: Professional Engineer / Technical Consultant\nExpertise Level: Advanced: 27 (75%) / Expert: 9 (25%)\nPrimary Domains: Aerospace/Drones (Commercial): 24 (67%), Defense/Government: 10 (28%), Research Institutions: 1 (3%), Other: 1 (2%)\n\n\n\n3.3.2 P2 - Junior Engineers\nProfessional engineers in commercial or defense settings who are building their drone and simulation expertise. This includes career switchers, new hires at drone companies, and engineers expanding their skillset to include simulation tools.\n\nProfession: Professional Engineer / Technical Consultant\nExpertise Level: Intermediate: 20 (69%), Novice: 9 (31%)\nPrimary Domains: Aerospace/Drones (Commercial): 18 (62%), Defense/Government: 6 (21%), Research Institutions: 2 (7%), Other: 3 (10%)\n\n\n\n3.3.3 P3 - Academic Researchers\nAcademic and industry researchers using PX4 simulators for cutting-edge research, publications, and experimental work. This group spans university researchers, PhD students/postdocs, and R&D engineers at companies and research institutions.\n\nProfession: Researcher (Academic/Industry)\nExpertise Level: Intermediate: 14 (50%), Advanced: 11 (39%), Expert: 2 (7%), Novice: 1 (4%)\nPrimary Domains: Academia/Education: 10 (36%), Aerospace/Drones (Commercial): 7 (25%), Research Institutions: 7 (25%), Defense/Government: 3 (11%), Aerospace/Drones (Hobbyist): 1 (3%)\n\n\n\n3.3.4 P4 - Academic Students\nUniversity students (BSc/MSc level) learning drone simulation as part of their coursework, research projects, or thesis work. This is the entry-level academic segment that represents the future pipeline of PX4 developers and users.\n\nProfession: Student (BSc/MSc)\nExpertise Level: Intermediate: 7 (39%), Novice: 8 (44%), Advanced: 2 (11%), Expert: 1 (6%)\nPrimary Domains: Academia/Education: 12 (67%), Aerospace/Drones (Hobbyist): 3 (17%), Aerospace/Drones (Commercial): 2 (11%), Other: 1 (5%)\n\n\n\n3.3.5 P5 - Independent Hobbyists\nDrone enthusiasts, hobbyists, and independent developers exploring PX4 for personal projects, skill development, or potential commercial opportunities. This small but passionate group contributes to community growth and grassroots adoption.\n\nProfession: Hobbyist / Independent Developer / Non-professional\nExpertise Level: Intermediate: 5 (63%), Novice: 2 (25%), Advanced: 1 (12%)\nPrimary Domains: Aerospace/Drones (Hobbyist): 6 (75%), Aerospace/Drones (Commercial): 1 (12%), Robotics: 1 (12%)"
  },
  {
    "objectID": "docs/index.html#participants-experience-with-simulation",
    "href": "docs/index.html#participants-experience-with-simulation",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.4 Participants Experience with Simulation",
    "text": "3.4 Participants Experience with Simulation\nThis section presents results on participants’ past simulation experience and currently used simulators, as well as data on simulated vehicle types and hardware specifications.\n\n\n\n\n\n\nFigure 4: Bar graph with simulators experienced vs last used simulators. Light red stands for an unclear amount of current users using Gazebo New (previous ignition) due to ambiguity in the answers.\n\n\n\n\n3.4.1 Simulation Past Experience\nParticipants listed all simulators they had previously used, as shown in Figure 4. The two most common were Gazebo (New) with 94 responses and Gazebo Classic with 79 responses, the two officially supported simulators for PX4 autopilot. Usage drops significantly after these, with Simulink (29), Isaac Sim (28), and AirSim (26) following. The “Other” category included X-Plane, Webots, CoppeliaSim, SIH, and several custom-built simulators.\n\n\n3.4.2 Simulation Current Use\nFigure 4 also shows current simulator usage based on an open-ended question. Due to ambiguity about which version of Gazebo participants were using, the following post-processing classifications were applied:\n\nParticipants with experience only in Gazebo Classic were classified as currently using Gazebo Classic\nParticipants with experience only in Gazebo New were classified as currently using Gazebo New\nParticipants who specified “gz sim,” version names (Harmonic/Ionic), or clarified in comments were classified as Gazebo New\n\nAfter this classification, 30 participants who had used both versions could not be definitively categorized. These are shown in light red in Figure 4, with the assumption they likely use the most current and recommended simulator (Gazebo New), though this metric should be interpreted with caution. Overall, Gazebo New remains the most widely used simulator (46 confirmed users), followed by Gazebo Classic (19). AirSim and its forks account for 5 current users, while Simulink and JSBSim each have 4 users.\n\n\n3.4.3 Summary of Comments on Gazebo New\nSince Gazebo New is the most widely used simulator, the following summarizes user feedback on its strengths and weaknesses. Users appreciate the versatility of Gazebo New and its seamless integration with both PX4 SITL and ROS 2. They value the wide variety of available sensors—a feature lacking in many other simulators—as well as its open-source nature and large community support. Users also noted significant improvements in the physics model compared to Gazebo Classic.\nHowever, users identified several areas needing improvement. Multiple participants reported instability on operating systems other than Ubuntu, particularly macOS. Many prefer Gazebo Classic’s GUI, describing it as less complex to work with. Adding custom sensors through the plugin framework and creating custom vehicle models pose significant challenges for many developers, particularly due to inadequate documentation. Beginners especially struggle to find appropriate documentation.\nAcademic researchers (P3) specifically noted that Gazebo is not well-suited for reinforcement learning applications, leading them to use alternative simulators for this purpose. They also requested better multi-vehicle simulation support, such as the ability to simplify or disable physics for certain vehicles. Additionally, while some users praised the updated physics, others reported that flight dynamics and environmental variables still fall short of requirements.\n\n\n3.4.4 Simulated Vehicles\nParticipants indicated which vehicle types and payloads they typically simulate (Figure 5). Quadcopters are the most commonly simulated vehicle type (104), followed by multicopters (52), with fixed-wing and VTOL vehicles tied for third (50 each). For payloads, cameras are most prevalent (103), followed by range sensors such as LiDAR/ToF sensors (88), with gimbals and communication equipment sharing third place.\nRegarding the number of vehicles simulated simultaneously, the majority simulate only one vehicle (63%), followed by those who typically simulate 5-10 vehicles (26%), and 7% who simulate teams of 2-4 vehicles (see Figure 6).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: (Left) Bar graph with the different types simulated vehicles and (right) bar graph with the distribution of type of payloads simulated.\n\n\n\n\n\n\n\n\n\nFigure 6: Pie-chart of the amount of simulated vehicles that participants usually simulate.\n\n\n\n\n\n3.4.5 Simulation Host Specifications\nThe survey also asked questions about the host computers running the simulation, including GPU-enabled systems, OS distribution, and whether the simulation runs locally or remotely. Figure 7 shows the hardware specifications participants use to run their simulators. The majority use PCs with dedicated GPUs, with Ubuntu as the dominant operating system. Most participants run simulators locally on their machines rather than on remote servers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Pie-charts with the (left) GPU-enabled machines, the OS distribution (middle) and running location of the simulation (right)."
  },
  {
    "objectID": "docs/index.html#purpose-and-goals",
    "href": "docs/index.html#purpose-and-goals",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.5 Purpose and Goals",
    "text": "3.5 Purpose and Goals\n\n3.5.1 Purpose for Using Simulators\nFigure 8 displays the reasons and purposes for using simulators. Participants most commonly use simulators for testing before real flights (89), followed by algorithm development (84) and robot prototyping and design (59).\nThe radar graph on the right of Figure 8 shows that both academic researchers and students also emphasize academic research and education/learning. Senior engineers (P1) strongly prioritize commercial product development, followed by junior engineers (P2) and independent developers/hobbyists (P5). Students (P4) and hobbyists (P5) also occasionally use simulation for competition preparation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: (Left) The bar graph with the count of indicated purposes of using simulators for all participants and (right) the radar graph with the purposes plotted per persona.\n\n\n\n\n\n3.5.2 Goals of Using Simulators\nThe bar graph on the left of Figure 9 shows that all participants use simulators as development tools for their robotic systems, followed by mission planning and validation (74) and quality assurance in continuous integration (48). The radar graph on the right reveals that senior engineers (P1) drive the high CI/QA scores, while academic researchers (P3) and independent hobbyists (P5) also use simulators for generating machine learning training data. A portion of students (P4) and hobbyists (P5) additionally indicated using simulation for safety testing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: (Left) A bar graph of the goals of using simulators for all participants and (right) the radar graph with goals plotted per persona."
  },
  {
    "objectID": "docs/index.html#features-and-priorities",
    "href": "docs/index.html#features-and-priorities",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.6 Features and Priorities",
    "text": "3.6 Features and Priorities\nParticipants rated the perceived importance of simulator features on a scale from 1 (least important) to 5 (very important) across four categories: technical capabilities, platform & compatibility, usability and accessibility, and advanced features. These ratings are also analyzed by persona, as defined in the previous section.\n\n3.6.1 Technical Capabilities\nTable 1 presents the importance ratings for technical capabilities. Software-in-the-loop (SITL) is considered most important with a score of 4.63, followed by realistic physics simulation (4.38) and hardware-in-the-loop (3.82). Junior engineers (P2) showed a slightly different preference profile, rating failure mode simulation (3.96) and simulated networking capabilities (4.00) higher than other technical capabilities beyond SITL.\n\n\n\nTable 1: The average perceived importance of technical capabilities of simulators per profile and for all participants overall. The scale goes from 0 (not important) to 5 (very important).\n\n\n\n\n\n\n\n\n3.6.2 Platform and Compatibility\nTable 2 shows that the highest-rated platform and compatibility feature is ROS/ROS2 integration (3.92), followed by network capabilities and version compatibility between different PX4 releases (both 3.82). Researchers (P3) prioritized this category more than any other persona profiles. Notable deviations include senior engineers (P1), who rated Docker/containerization support (3.81) higher than network capabilities, and independent developers/hobbyists (P5), who rated network capabilities significantly higher than all other platform and compatibility features.\n\n\n\nTable 2: The average perceived importance of platform and compatibility of simulators per profile and for all participants overall.\n\n\n\n\n\n\n\n\n3.6.3 Usability and Accessibility\nTable 3 presents importance ratings for usability and accessibility features. Participants rated template and example library availability as most important (4.14), followed by real-time debugging support and easy installation process. Academic students (P4) prioritized this category more than any other persona. Academic researchers (P3), students (P4), and independent developers/hobbyists (P5) all rated real-time parameter tuning higher than easy installation process, deviating from the overall ranking.\n\n\n\nTable 3: The average perceived importance of usability and accessibility of simulators per profile and for all participants overall.\n\n\n\n\n\n\n\n\n3.6.4 Advanced Features\nTable 4 lists the importance ratings for advanced simulation features. Across all participants, custom sensor model creation was rated most important (4.17), followed by easy robot model creation (4.13) and scenario scripting and automation (3.94). Academic researchers (P3) prioritized this category more than any other persona. Ratings were relatively consistent across personas, with the notable exception that independent developers/hobbyists (P5) also rated swarm simulation as highly important.\n\n\n\nTable 4: The average perceived importance of advanced features of simulation per profile and for all participants overall.\n\n\n\n\n\n\n\n\n3.6.5 Top Wanted Functionalities\nAcross all personas, the top 5 most important features are:\n\nSoftware-in-the-loop support\nRealistic physics simulation\nCustom sensor model creation\nTemplate/example library availability\nEasy robot model creation\n\nTable 5 breaks down the top 5 features by persona. Software-in-the-loop capabilities are considered essential by all personas except students (P4), who prioritize template and example library availability and real-time parameter tuning more highly. Realistic physics simulation is universally valued, though independent developers/hobbyists (P5) rated network capabilities and template availability higher. ROS/ROS2 integration is more important to advanced simulator users (senior engineers (P1) and academic researchers (P3)), while both junior (P2) and senior engineers (P1) rated custom sensor creation and easy robot model creation as equally important.\n\n\n\nTable 5: Top 5 wanted features per profile."
  },
  {
    "objectID": "docs/index.html#final-choice-for-priority-roadmap",
    "href": "docs/index.html#final-choice-for-priority-roadmap",
    "title": "PX4 Simulation Integration Survey",
    "section": "3.7 Final Choice for Priority Roadmap",
    "text": "3.7 Final Choice for Priority Roadmap\nIn a final single-choice question, participants selected the one feature they would prioritize if only one item could be on the roadmap (see Figure 10). The largest group (18%) requested improvements to the current Gazebo integration with PX4 autopilot. Another 17% prioritized better documentation and examples for existing simulators, while 16% wanted more realistic physics and higher-quality sensor models.\n\n\n\n\n\n\nFigure 10: Pie-chart with the distribution of the final priority question of all participants.\n\n\n\nFigure 11 presents a bar chart showing how each persona influenced the roadmap priorities. Professional engineers, both senior (P1) and junior (P2), voted for improved Gazebo integration with PX4. Academic researchers (P3) prioritized more realistic sensor and physics models along with better multi-vehicle/swarm simulation support. A significant portion of senior engineers (P1) also voted for reducing the sim-to-real gap, while some advocated for a new PX4-optimized simulator. Junior engineers (P2) emphasized enhanced simulation creation capabilities. Both junior engineers (P2) and academic students (P4), representing novice and intermediate users, indicated they would benefit most from better documentation and examples for existing PX4 simulation integrations. Finally, both senior engineers (P1) and academic students (P4) requested improved ROS/ROS2 integration with PX4 simulation.\n\n\n\n\n\n\nFigure 11: Bar chart to indicate the differences per persona on the final roadmap priority choice of the survey."
  },
  {
    "objectID": "docs/index.html#reflections-on-the-survey-results",
    "href": "docs/index.html#reflections-on-the-survey-results",
    "title": "PX4 Simulation Integration Survey",
    "section": "4.1 Reflections on the Survey Results",
    "text": "4.1 Reflections on the Survey Results\nOverall, participants agreed that software-in-the-loop support is essential and must remain a priority. While there were few direct comments about SITL functionality itself, several respondents noted that it was unclear what happens in the background or how to adjust features and sensors. This suggests the SITL documentation could benefit from review.\nOne surprising result was the difference in perceived importance between realistic physics models and photorealistic rendering. These two features were nearly opposite on the importance scale. This is unexpected given that AirSim and Isaac Sim, both celebrated for their rendering capabilities, were mentioned as popular alternatives to Gazebo. However, users of these simulators represented only a small portion of respondents. An interesting suggestion from the survey was to explore JSBSim integration for flight dynamics in Gazebo New. That said, it is difficult to evaluate “how realistic” physics models need to be without widely accepted benchmarks for simulator evaluation.\nDocumentation emerged as a significant pain point. Many participants reported difficulty finding proper documentation, though they did not always specify where. Several mentions pointed to Gazebo’s physics plugins, such as the multicopter plugin, environment plugins like wind, and physics integration, as being poorly documented. These are areas where the PX4 community could contribute directly to Gazebo.\nA clear difference emerged between the needs of more experienced developers (P1 and P3) and those with limited experience (P2, P4). The latter group showed stronger preference for better examples and tutorials, which is also reflected in the feature importance scores. Senior engineers and researchers expressed more interest in improving simulator realism, primarily related to physics. Senior engineers (P1) additionally emphasized improving the overall PX4-Gazebo integration.\nRegarding which simulator to prioritize: Gazebo New is both the most used and the most requested for improvement. A substantial portion of users (the second-largest group) still relies on Gazebo Classic despite growing compatibility issues. Since Gazebo Classic is end-of-life, there will be limited support from the maintenance team to address these. For issues with Gazebo New, this survey lacked the granularity to create specific tickets, so additional effort is needed within the community to properly document problems and create minimal reproducible examples."
  },
  {
    "objectID": "docs/index.html#recommendation",
    "href": "docs/index.html#recommendation",
    "title": "PX4 Simulation Integration Survey",
    "section": "4.2 Recommendation",
    "text": "4.2 Recommendation\nThe following recommendations are offered by this report’s author; final decisions should be made in consultation with the PX4 simulation maintenance team based on their availability and capabilities.\nBased on the survey results, the recommendation is not to focus efforts on building a new simulator or integrating with community simulators like AirSim forks or Isaac Sim. Instead, development efforts should focus solely on improving the current PX4 and Gazebo New integration. Gazebo Classic should no longer be recommended, as it is end-of-life and out of scope.\nThe most immediate opportunity is improving documentation. The quick-start guide appears sufficient for most users, as basic setup runs mostly out-of-the-box. However, users struggle to understand the underlying workflows. When junior developers and students need to customize their frame or model, they encounter difficulties due to a lack of examples. These could be developed through structured efforts similar to ROS’s and Gazebo’s tutorial parties.\nFor advanced developers, there is a need for better troubleshooting guides and more comprehensive API documentation-contributions that would also benefit Gazebo’s plugin documentation. Additionally, a version compatibility matrix documenting tested combinations of ROS, PX4, and Gazebo versions would address a frequently cited frustration. Given the complexity of these version combinations, it must remain possible for developers to contribute fixes and documentation to either the PX4 or Gazebo teams.\nThe next priority should be custom model and sensor creation, which was consistently mentioned as a pain point. This is challenging both from the Gazebo side and the PX4 integration side. Junior engineers would benefit most from improvements here.\nFinally, there is clear interest in better physics simulation. However, as in aerial robotics more broadly, it is difficult to define what “better physics” means without knowing the application. Swarm simulations may require minimal physics fidelity, while a quadcopter carrying a payload demands much more. An investigation into benchmarking techniques (perhaps drawn from the game development industry) could help establish evaluation criteria for physics simulation quality."
  },
  {
    "objectID": "docs/index.html#about-this-survey",
    "href": "docs/index.html#about-this-survey",
    "title": "PX4 Simulation Integration Survey",
    "section": "6.1 About This Survey",
    "text": "6.1 About This Survey\nWe’re gathering feedback from the PX4 community to inform our simulation integration roadmap. Your input will help us prioritize which simulators to support, what features to develop, and where to focus our documentation efforts.\nWho should take this survey: - Anyone who has used or is interested in using simulation with PX4 - Users of any experience level, from beginners to experts - Whether you’re in academia, industry, or working as a hobbyist\nYour responses are anonymous. We’ll only collect email addresses if you volunteer for follow-up interviews.\n\n\nGetting to Know You\n\nWe’d like to understand your background and experience level to better contextualize your feedback.\n1. What is your primary profession?\n\nStudent (BSc/MSc)\nResearcher (Academic/Industry)\nEducator/Teacher\nProfessional Engineer / Technical Consultant\nHobbyist / Independent Developer\nOther: ___________\n\n2. What domain are you working in?\n\nAcademia/Education\nAerospace/Drones (Commercial)\nAerospace/Drones (Hobbyist)\nDefense/Government\nResearch Institution\nOther: ___________\n\n3. What is your level of expertise regarding robotic simulators?\n\nNovice\nIntermediate\nAdvanced\nExpert\n\n4. How long have you been working with PX4?\n\nI haven’t worked with PX4\nI just started working with PX4 (&lt; 1 month)\n1 month - 1 year\n1-3 years\n3-5 years\n5+ years\n\n\n\nCurrent Simulator Usage\n\nThis section helps us understand which tools you’re using and how you’re using them in your current workflow.\n5. Which simulators have you worked with in your setup? (Select all that apply)\n\nNvidia Isaac Sim\nGazebo (formerly known as Ignition)\nGazebo Classic\nJMAVSim\nAirSim\nMuJoCo\nPybullet\nFlightGear\nJSBSim\nSimulink\nOther: ___________\n\n6. What was your primary purpose for using simulators? (Select all that apply)\n\nAcademic research\nEducation/learning\nRobot prototyping/design\nAlgorithm development\nTesting before real flights\nDemonstration/presentation\nCompetition preparation\nCommercial product development\nFlight tooling design\nOther: ___________\n\n7. For which goals are you using simulators? (Select all that apply)\n\nDevelopment tool for robotic systems\nQuality assurance in continuous integration\nData generation for machine learning\nMission planning and validation\nTraining pilots/operators\nSafety\nRegulatory compliance testing\nOther: ___________\n\n8. What type of aerial robotic vehicle are you working with? (Select all that apply)\n\nHelicopter\nQuadcopter\nMulticopters (more than 4 propellers)\nFixed wing\nHybrid / VTOL vehicles\nBlimps\nOther: ___________\n\n9. Does your platform have additional equipment or payloads? (Select all that apply)\n\nManipulator arm\nGripper/claw\nGimbal\nCamera\nCargo hook/winch\nPerception Sensors (lidar, sonar, etc.)\nCommunication equipment\nNone of the above\nOther: ___________\n\n10. How many robots/UAVs do you typically simulate simultaneously?\n\nJust one (1)\n2-4\n5-10\n10-100\n100+\n\n11. How do you typically interact with your simulator? (Select all that apply)\n\nMainly through GUI\nHeadless/CI environments\nCustom plugins/extensions\nROS/ROS2 integration\nAPI/scripting\nOther: ___________\n\n12. What’s do you like the most with current/latest simulator you are using?\n\n13. What’s your biggest frustration with current/latest simulator you are using?"
  },
  {
    "objectID": "docs/index.html#section",
    "href": "docs/index.html#section",
    "title": "PX4 Simulation Integration Survey",
    "section": "6.2 ____",
    "text": "6.2 ____\n\nTechnical Setup & Constraints\n\nUnderstanding your hardware setup and limitations helps us design tools that work for everyone’s environment.\n14. Did you have to purchase/upgrade your computer specifically for simulation?\n\nYes, bought entirely new system\nYes, upgraded existing system\nNo, used existing hardware\nUsing cloud/remote computing\nOther: ___________\n\n15. What operating system do you primarily use for simulation?\n\nUbuntu\nArch Linux\nFedora Linux\nWindows\nMacOS\nOther: ___________\n\n16. What operating system version are you using for the above selection?\n\n17. Do you run simulations locally or remotely?\n\nLocally on my machine\nOn remote machines\nIn the cloud\nOther: ___________\n\n18. Is your machine on which you run your simulation GPU-enabled?\n\nYes\nNo\nNot sure\n\n19. What’s your biggest hardware limitation?\n\nCPU performance\nGPU/graphics performance\nRAM capacity\nStorage space\nNetwork bandwidth\nNo significant limitations\nOther: ___________\n\n20. Anything to add to any of the questions answered in this section regarding computer specifications?\n\n\n\nDesired Features & Priorities\n\nThis section helps us prioritize development efforts by understanding which features matter most to different types of users.\nPlease fill in the survey based on your first instinct (don’t linger too long per question)\n21. Please rate the preferred Technical Capabilities\nRate on scale: 1 (Not important) to 5 (Very important), or No preference/opinion\n\nHardware-in-the-loop support\nSoftware-in-the-loop support\nFaster than real-time simulation\nPhotorealistic rendering\nRealistic physics simulation\nWeather/environmental simulation\nFailure mode simulation\nCommunication simulation\n\n22. Please rate the preferred Platform & Compatibility\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nMultiple OS support\nROS/ROS 2 integration\nCloud simulation capabilities\nDocker/containerization support\nNetwork capability (PX4 and simulator on different machines)\nCompatibility between different versions of PX4 and simulators\n\n23. Please rate the preferred Usability & Accessibility\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nBeginner-friendly interface\nEasy installation process\nTemplate/example library availability\nReal-time parameter tuning\nReal-time debug support (pause, step through simulation)\nSimulation snapshot/hotstart capability\n\n24. Please rate the preferred Advanced Features\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nMulti-agent/swarm simulation\nMachine learning integration\nCustom sensor model creation\nEasy robot model creation\nScenario scripting/automation\nPerformance benchmarking tools\nCAD program integration for custom models\n\n25. Any more features that we should pay attention to as well?\n\n\n\nPX4 Simulation Roadmap\n\nThis section helps us understand what strategic direction would best serve the PX4 community.\n26. If there was one thing you’d like the PX4 simulation development team to work on, what would it be?\n\nImproving the current Gazebo simulator intergration\nA new PX4-optimized simulator\nAdd support for more simulators for PX4\nBetter documentation and examples for existing PX4 simulation\nMore simulation model creation capabilities (more types of vehicles)\nMore realistic sensor/physics models\nImprove stability/performance of existing simulator integrations\nBetter PX4-simulator integration (easier SITL setup, better parameter mapping)\nMore realistic sensor/physics models (Sim2Real gap)\nBetter multi-vehicle/swarm simulation support\nImproved ROS/ROS2 integration with PX4 simulation\nFixing PX4 - Simulator version compatibility issues\nOther\n\n27. Why is this choice your top priority?\n\n\n\nFinal Questions\n\nFinal thoughts and opportunities for deeper engagement with our improvement efforts.\n28. Any additional comments or suggestions?\n\n29. Would you be willing to participate in follow-up interviews? If yes, please provide an email address where we can contact you. Your email address will only be used for this exact purpose.\n\n\n\nThank you!\n\nThank you for taking the time to share your experiences! Your feedback will help improve simulation tools for the entire PX4 community.\nPlease press next to submit the form"
  },
  {
    "objectID": "docs/survey_content_final.html",
    "href": "docs/survey_content_final.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/survey_content_final.html#about-this-survey",
    "href": "docs/survey_content_final.html#about-this-survey",
    "title": "",
    "section": "1.1 About This Survey",
    "text": "1.1 About This Survey\nWe’re gathering feedback from the PX4 community to inform our simulation integration roadmap. Your input will help us prioritize which simulators to support, what features to develop, and where to focus our documentation efforts.\nWho should take this survey: - Anyone who has used or is interested in using simulation with PX4 - Users of any experience level, from beginners to experts - Whether you’re in academia, industry, or working as a hobbyist\nYour responses are anonymous. We’ll only collect email addresses if you volunteer for follow-up interviews.\n\n\nGetting to Know You\n\nWe’d like to understand your background and experience level to better contextualize your feedback.\n1. What is your primary profession?\n\nStudent (BSc/MSc)\nResearcher (Academic/Industry)\nEducator/Teacher\nProfessional Engineer / Technical Consultant\nHobbyist / Independent Developer\nOther: ___________\n\n2. What domain are you working in?\n\nAcademia/Education\nAerospace/Drones (Commercial)\nAerospace/Drones (Hobbyist)\nDefense/Government\nResearch Institution\nOther: ___________\n\n3. What is your level of expertise regarding robotic simulators?\n\nNovice\nIntermediate\nAdvanced\nExpert\n\n4. How long have you been working with PX4?\n\nI haven’t worked with PX4\nI just started working with PX4 (&lt; 1 month)\n1 month - 1 year\n1-3 years\n3-5 years\n5+ years\n\n\n\nCurrent Simulator Usage\n\nThis section helps us understand which tools you’re using and how you’re using them in your current workflow.\n5. Which simulators have you worked with in your setup? (Select all that apply)\n\nNvidia Isaac Sim\nGazebo (formerly known as Ignition)\nGazebo Classic\nJMAVSim\nAirSim\nMuJoCo\nPybullet\nFlightGear\nJSBSim\nSimulink\nOther: ___________\n\n6. What was your primary purpose for using simulators? (Select all that apply)\n\nAcademic research\nEducation/learning\nRobot prototyping/design\nAlgorithm development\nTesting before real flights\nDemonstration/presentation\nCompetition preparation\nCommercial product development\nFlight tooling design\nOther: ___________\n\n7. For which goals are you using simulators? (Select all that apply)\n\nDevelopment tool for robotic systems\nQuality assurance in continuous integration\nData generation for machine learning\nMission planning and validation\nTraining pilots/operators\nSafety\nRegulatory compliance testing\nOther: ___________\n\n8. What type of aerial robotic vehicle are you working with? (Select all that apply)\n\nHelicopter\nQuadcopter\nMulticopters (more than 4 propellers)\nFixed wing\nHybrid / VTOL vehicles\nBlimps\nOther: ___________\n\n9. Does your platform have additional equipment or payloads? (Select all that apply)\n\nManipulator arm\nGripper/claw\nGimbal\nCamera\nCargo hook/winch\nPerception Sensors (lidar, sonar, etc.)\nCommunication equipment\nNone of the above\nOther: ___________\n\n10. How many robots/UAVs do you typically simulate simultaneously?\n\nJust one (1)\n2-4\n5-10\n10-100\n100+\n\n11. How do you typically interact with your simulator? (Select all that apply)\n\nMainly through GUI\nHeadless/CI environments\nCustom plugins/extensions\nROS/ROS2 integration\nAPI/scripting\nOther: ___________\n\n12. What’s do you like the most with current/latest simulator you are using?\n\n13. What’s your biggest frustration with current/latest simulator you are using?"
  },
  {
    "objectID": "docs/survey_content_final.html#section",
    "href": "docs/survey_content_final.html#section",
    "title": "",
    "section": "1.2 ____",
    "text": "1.2 ____\n\nTechnical Setup & Constraints\n\nUnderstanding your hardware setup and limitations helps us design tools that work for everyone’s environment.\n14. Did you have to purchase/upgrade your computer specifically for simulation?\n\nYes, bought entirely new system\nYes, upgraded existing system\nNo, used existing hardware\nUsing cloud/remote computing\nOther: ___________\n\n15. What operating system do you primarily use for simulation?\n\nUbuntu\nArch Linux\nFedora Linux\nWindows\nMacOS\nOther: ___________\n\n16. What operating system version are you using for the above selection?\n\n17. Do you run simulations locally or remotely?\n\nLocally on my machine\nOn remote machines\nIn the cloud\nOther: ___________\n\n18. Is your machine on which you run your simulation GPU-enabled?\n\nYes\nNo\nNot sure\n\n19. What’s your biggest hardware limitation?\n\nCPU performance\nGPU/graphics performance\nRAM capacity\nStorage space\nNetwork bandwidth\nNo significant limitations\nOther: ___________\n\n20. Anything to add to any of the questions answered in this section regarding computer specifications?\n\n\n\nDesired Features & Priorities\n\nThis section helps us prioritize development efforts by understanding which features matter most to different types of users.\nPlease fill in the survey based on your first instinct (don’t linger too long per question)\n21. Please rate the preferred Technical Capabilities\nRate on scale: 1 (Not important) to 5 (Very important), or No preference/opinion\n\nHardware-in-the-loop support\nSoftware-in-the-loop support\nFaster than real-time simulation\nPhotorealistic rendering\nRealistic physics simulation\nWeather/environmental simulation\nFailure mode simulation\nCommunication simulation\n\n22. Please rate the preferred Platform & Compatibility\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nMultiple OS support\nROS/ROS 2 integration\nCloud simulation capabilities\nDocker/containerization support\nNetwork capability (PX4 and simulator on different machines)\nCompatibility between different versions of PX4 and simulators\n\n23. Please rate the preferred Usability & Accessibility\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nBeginner-friendly interface\nEasy installation process\nTemplate/example library availability\nReal-time parameter tuning\nReal-time debug support (pause, step through simulation)\nSimulation snapshot/hotstart capability\n\n24. Please rate the preferred Advanced Features\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nMulti-agent/swarm simulation\nMachine learning integration\nCustom sensor model creation\nEasy robot model creation\nScenario scripting/automation\nPerformance benchmarking tools\nCAD program integration for custom models\n\n25. Any more features that we should pay attention to as well?\n\n\n\nPX4 Simulation Roadmap\n\nThis section helps us understand what strategic direction would best serve the PX4 community.\n26. If there was one thing you’d like the PX4 simulation development team to work on, what would it be?\n\nImproving the current Gazebo simulator intergration\nA new PX4-optimized simulator\nAdd support for more simulators for PX4\nBetter documentation and examples for existing PX4 simulation\nMore simulation model creation capabilities (more types of vehicles)\nMore realistic sensor/physics models\nImprove stability/performance of existing simulator integrations\nBetter PX4-simulator integration (easier SITL setup, better parameter mapping)\nMore realistic sensor/physics models (Sim2Real gap)\nBetter multi-vehicle/swarm simulation support\nImproved ROS/ROS2 integration with PX4 simulation\nFixing PX4 - Simulator version compatibility issues\nOther\n\n27. Why is this choice your top priority?\n\n\n\nFinal Questions\n\nFinal thoughts and opportunities for deeper engagement with our improvement efforts.\n28. Any additional comments or suggestions?\n\n29. Would you be willing to participate in follow-up interviews? If yes, please provide an email address where we can contact you. Your email address will only be used for this exact purpose.\n\n\n\nThank you!\n\nThank you for taking the time to share your experiences! Your feedback will help improve simulation tools for the entire PX4 community.\nPlease press next to submit the form"
  },
  {
    "objectID": "docs/preperation_survey.html",
    "href": "docs/preperation_survey.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/preperation_survey.html#survey-design-and-methodology",
    "href": "docs/preperation_survey.html#survey-design-and-methodology",
    "title": "",
    "section": "1.1 Survey Design and Methodology",
    "text": "1.1 Survey Design and Methodology\n\n1.1.1 Survey Structure\nThe PX4 Simulator Community Survey was designed to gather feedback from users across all aspects of their simulation experience. The survey is set up in six main sections: demographic background, current simulator usage patterns, technical infrastructure, simulator experience evaluation, feature priorities, and strategic recommendations. The survey uses a mixed-methods approach combining quantitative Likert-scale ratings with qualitative open-ended questions.\nThe survey begins by establishing user segments through profession, simulator expertise level, and PX4 experience duration. This segmentation allows analysis of how different user groups (students, researchers, commercial engineers) experience simulation and what improvements they prioritize.\nQuestions about which simulators users employ, their purposes (research, testing, commercial development), vehicle types, and scale (single vs. multi-vehicle) help map the current PX4 simulation landscape. This information provides insight into how the community currently uses simulation tools.\nHardware and infrastructure questions examine potential barriers to adoption. The survey asks whether users have purchased new systems, what operating systems they use, and where they encounter limitations (CPU, GPU, RAM). This data provides information about current hardware requirements and performance constraints.\nThe survey uses detailed Likert-scale assessments across five dimensions: installation experience, user interface, technical performance, feature support, and community resources. This structured approach allows users to provide feedback on specific aspects of their simulation experience. Open-ended questions at the end allow participants to share observations that may not be captured by the structured questions.\nUsers rate importance of potential improvements across technical capabilities, platform compatibility (OS support, ROS integration), usability, and advanced features. This provides a prioritization matrix showing which capabilities matter most to different segments of the community.\nThe survey concludes with open-ended questions soliciting input on the most important improvements needed and offering opportunities for follow-up interviews. This acknowledges that while quantitative data identifies patterns, qualitative discussion often reveals underlying causes and context that ratings alone cannot capture.\nThe pilot version of the survey can be found in the Appendix ( ?@sec-pilot )"
  },
  {
    "objectID": "docs/preperation_survey.html#pilot-survey-analysis",
    "href": "docs/preperation_survey.html#pilot-survey-analysis",
    "title": "",
    "section": "1.2 Pilot Survey Analysis",
    "text": "1.2 Pilot Survey Analysis\nA pilot survey was conducted with 6 PX4 developers to validate the survey instrument before wider distribution. Respondents were primarily professional engineers with advanced to expert-level simulator expertise, and most have 5+ years of PX4 experience. The results of this pilot survey will be used to fine-tune the final survey before it is sent out to the rest of the community.\n\n1.2.1 Simulator Usage and Goals\nAll pilot participants use Gazebo (Classic or Harmonic). About half also use AirSim, FlightGear, or JSBSim. Primary use cases center on algorithm development, pre-flight testing, and quality assurance in CI pipelines. All respondents simulate only single vehicles, which differs from forum analysis (Section 1.3) where multi-vehicle simulation appears as a recurring topic in the broader community. Vehicle types are dominated by quadcopters, followed by fixed-wing aircraft, with VTOL and specialized vehicles showing lower representation. The equipment carried by these quadcopters is predominantly cameras and perception sensors.\n\n\n1.2.2 Technical Infrastructure\nAlmost all pilot participants had purchased new systems or upgraded existing ones specifically for simulation. Ubuntu dominates the operating system distribution with minimal Windows usage. Despite substantial hardware investment, half report no significant limitations.\n\n\n1.2.3 Current Simulator Experience\nRespondents evaluated their most recent simulator (primarily Gazebo variants) across multiple dimensions. Installation ease received mixed feedback. The overall user experience for experienced developers appeared positive, though opinions on documentation and tutorials quality varied considerably. It is unclear whether respondents were referring to PX4 documentation or simulator documentation. Several comments addressed physics realism, particularly in Gazebo itself. While sensor variety was received positively, sensor models and physics simulation received negative feedback from multiple respondents, with some linking this directly to simulator usefulness. Several open-ended responses mentioned PX4 versioning issues with simulators, noting difficulty in determining compatible version combinations. This observation aligns with patterns observed in the forum analysis (found in Section 1.3).\n\n\n1.2.4 Desired Features and Priority\nIn the Desired Features and Priority section, pilot participants rated several items as very important. Software-in-the-loop (SITL) received higher importance ratings than Hardware-in-the-loop (HITL). ROS/ROS2 integration showed clear consensus as essential. Participants also indicated preference for Docker container support.\nRealistic physics models received higher importance ratings than photorealistic rendering, despite most pilot participants using cameras as main sensors. Multi-agent simulation was rated as very important by half of the pilot participants, though they indicated typically simulating only single vehicles. Communication/network simulation received varied responses.\nParticipants showed preference for the ability to create custom sensor models and easily create robot models. GUI features such as drag-and-drop capabilities or beginner-friendly interfaces scored lower on importance ratings."
  },
  {
    "objectID": "docs/preperation_survey.html#sec-forum",
    "href": "docs/preperation_survey.html#sec-forum",
    "title": "",
    "section": "1.3 Forum Analysis: PX4 Simulation Integration Issues",
    "text": "1.3 Forum Analysis: PX4 Simulation Integration Issues\nThis analysis examines 152 forum posts from the PX4 discussion forum (discuss.px4.io) spanning September 2024 through September 2025 to identify pressing issues in simulation integration. Several representative forum messages are referenced throughout this text, but the full list of forum posts evaluated is available upon request.\nSeveral categories emerged from this investigation, which will be discussed here.\n\n1.3.1 Version Stability and Platform Support\nForum posts indicate that some PX4 updates have introduced breaking changes. Users report that critical parameters for custom spawn positions were removed without alternatives[1], and Hardware-in-the-Loop functionality broke in newer versions[2]. Platform-specific issues appear more frequently for macOS users, especially those on Apple Silicon[3].\n\n\n1.3.2 Core Simulation Reliability\nUsers report problems with simulation accuracy and stability. Some posts describe Extended Kalman Filter position drift exceeding 50 meters during simple hovering[4], particularly when using custom worlds or external sensors. Flight dynamics issues appear in multiple posts: drones arm correctly but fail to take off, or cannot climb beyond minimal altitudes[5], [6]. One post identifies an architectural limitation preventing custom simulators from running slower than real-time[7].\n\n\n1.3.3 Multi-Vehicle Scalability\nFor multi-vehicle simulation, users report startup times exceeding 20 minutes for multiple drones[8]. Configuration complexity appears in multiple posts, including managing unique ports, distinguishing sensor topics, and coordinating spawn positions[9], [10], [11]. Posts also describe communication failures between ground control stations and multiple vehicles[12], [13].\n\n\n1.3.4 Ecosystem Integration\nMultiple posts describe ROS2 bridge reliability issues where sensor data visible in Gazebo fails to reach ROS2 topics[14], [15]. Offboard control through ROS2 shows inconsistent behavior in several posts, including frame convention problems and actuator control failures[16], [17]. Beyond ROS2, posts describe difficulties integrating alternative simulators like Unity, AirSim, and MATLAB/Simulink[18], [19], [20].\n\n\n1.3.5 Custom Model Development\nPosts describe unpredictable flight behavior when modifying physical properties[21], and unclear physical meaning in the motor control interface[22]. Specialized vehicle types (VTOLs, coaxial configurations, fixed-wing aircraft) appear in several posts describing issues, including dangerous behaviors like uncontrolled dives during transitions[23], [24].\n\n\n1.3.6 Sensor and Environment Fidelity\nPosts describe camera streams failing to initialize properly[25], documented features like terrain following not working[26], and environmental effects such as wind failing to manifest despite available commands[27]. Multiple posts report connection failures with ground control stations when using custom worlds[28], with some requiring undocumented workarounds[29].\n\n\n1.3.7 Deployment and Containerization\nPosts report Docker-based deployment issues including connection failures, missing video feeds, and rendering problems[30], [31], [32]."
  },
  {
    "objectID": "docs/preperation_survey.html#changes-and-additions-for-final-version-survey",
    "href": "docs/preperation_survey.html#changes-and-additions-for-final-version-survey",
    "title": "",
    "section": "1.4 Changes and Additions for Final Version Survey",
    "text": "1.4 Changes and Additions for Final Version Survey\nThe final version of the PX4 Simulator Community Survey incorporates changes based on pilot feedback and forum analysis. The survey was reduced from approximately 35 questions with over 70 individual items to 29 questions with fewer than 40 items for participants to answer.\n\n1.4.1 Survey Length and Structure\nThe most significant change was removing the entire “Current Simulator Experience” section, which contained 30+ Likert-scale items asking respondents to rate installation ease, user experience, performance, and documentation. Pilot respondents mentioned survey length concerns, and these quantitative ratings showed less consensus compared to open-ended feedback. The survey now retains two open-ended questions asking what users like most and their biggest frustration. The Gazebo team will be conducting their own survey on Gazebo simulator issues separately.\n\n\n1.4.2 Audience Expansion\nThe pilot survey participants were primarily experienced, industrial-oriented developers, all simulating only single vehicles. Forum analysis showed issues (installation problems, multi-vehicle challenges, documentation gaps) that appear to affect novice and intermediate users. The final version adds explicit language in the introduction stating the survey is for “anyone who has used or is interested in using simulation with PX4” and adds “Hobbyist / Independent Developer” as a profession option.\n\n\n1.4.3 Version Compatibility and Integration\nForum analysis (Section 1.3) revealed version compatibility as a recurring topic, with posts describing breaking changes between PX4 releases removing critical features like custom spawn positions. Multiple users expressed frustration that “things worked in one configuration, but does not in another.” The final survey adds explicit questions about version compatibility issues and importance.\n\n\n1.4.4 Multi-Vehicle and Advanced Features\nWhile pilot respondents all simulated single vehicles, forum analysis identified multi-vehicle simulation as a recurring topic. The final survey retains questions about the importance of multi-vehicle simulation and networking communication simulation.\n\n\n1.4.5 Strategic Roadmap Priority\nA new section asks respondents to select their single top priority for PX4 simulation development and justify why. This forced-choice question aims to identify what participants consider the most important issue in their workflow regarding PX4 simulation integration.\nThe final survey can be found in the Appendix ( ?@sec-final )"
  },
  {
    "objectID": "docs/conclusion.html",
    "href": "docs/conclusion.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/conclusion.html#reflections-on-the-survey-results",
    "href": "docs/conclusion.html#reflections-on-the-survey-results",
    "title": "",
    "section": "1.1 Reflections on the Survey Results",
    "text": "1.1 Reflections on the Survey Results\nOverall, participants agreed that software-in-the-loop support is essential and must remain a priority. While there were few direct comments about SITL functionality itself, several respondents noted that it was unclear what happens in the background or how to adjust features and sensors. This suggests the SITL documentation could benefit from review.\nOne surprising result was the difference in perceived importance between realistic physics models and photorealistic rendering. These two features were nearly opposite on the importance scale. This is unexpected given that AirSim and Isaac Sim, both celebrated for their rendering capabilities, were mentioned as popular alternatives to Gazebo. However, users of these simulators represented only a small portion of respondents. An interesting suggestion from the survey was to explore JSBSim integration for flight dynamics in Gazebo New. That said, it is difficult to evaluate “how realistic” physics models need to be without widely accepted benchmarks for simulator evaluation.\nDocumentation emerged as a significant pain point. Many participants reported difficulty finding proper documentation, though they did not always specify where. Several mentions pointed to Gazebo’s physics plugins, such as the multicopter plugin, environment plugins like wind, and physics integration, as being poorly documented. These are areas where the PX4 community could contribute directly to Gazebo.\nA clear difference emerged between the needs of more experienced developers (P1 and P3) and those with limited experience (P2, P4). The latter group showed stronger preference for better examples and tutorials, which is also reflected in the feature importance scores. Senior engineers and researchers expressed more interest in improving simulator realism, primarily related to physics. Senior engineers (P1) additionally emphasized improving the overall PX4-Gazebo integration.\nRegarding which simulator to prioritize: Gazebo New is both the most used and the most requested for improvement. A substantial portion of users (the second-largest group) still relies on Gazebo Classic despite growing compatibility issues. Since Gazebo Classic is end-of-life, there will be limited support from the maintenance team to address these. For issues with Gazebo New, this survey lacked the granularity to create specific tickets, so additional effort is needed within the community to properly document problems and create minimal reproducible examples."
  },
  {
    "objectID": "docs/conclusion.html#recommendation",
    "href": "docs/conclusion.html#recommendation",
    "title": "",
    "section": "1.2 Recommendation",
    "text": "1.2 Recommendation\nThe following recommendations are offered by this report’s author; final decisions should be made in consultation with the PX4 simulation maintenance team based on their availability and capabilities.\nBased on the survey results, the recommendation is not to focus efforts on building a new simulator or integrating with community simulators like AirSim forks or Isaac Sim. Instead, development efforts should focus solely on improving the current PX4 and Gazebo New integration. Gazebo Classic should no longer be recommended, as it is end-of-life and out of scope.\nThe most immediate opportunity is improving documentation. The quick-start guide appears sufficient for most users, as basic setup runs mostly out-of-the-box. However, users struggle to understand the underlying workflows. When junior developers and students need to customize their frame or model, they encounter difficulties due to a lack of examples. These could be developed through structured efforts similar to ROS’s and Gazebo’s tutorial parties.\nFor advanced developers, there is a need for better troubleshooting guides and more comprehensive API documentation-contributions that would also benefit Gazebo’s plugin documentation. Additionally, a version compatibility matrix documenting tested combinations of ROS, PX4, and Gazebo versions would address a frequently cited frustration. Given the complexity of these version combinations, it must remain possible for developers to contribute fixes and documentation to either the PX4 or Gazebo teams.\nThe next priority should be custom model and sensor creation, which was consistently mentioned as a pain point. This is challenging both from the Gazebo side and the PX4 integration side. Junior engineers would benefit most from improvements here.\nFinally, there is clear interest in better physics simulation. However, as in aerial robotics more broadly, it is difficult to define what “better physics” means without knowing the application. Swarm simulations may require minimal physics fidelity, while a quadcopter carrying a payload demands much more. An investigation into benchmarking techniques (perhaps drawn from the game development industry) could help establish evaluation criteria for physics simulation quality."
  },
  {
    "objectID": "docs/results.html",
    "href": "docs/results.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/results.html#professions-and-domains",
    "href": "docs/results.html#professions-and-domains",
    "title": "",
    "section": "1.1 Professions and Domains",
    "text": "1.1 Professions and Domains\nThe survey revealed that the largest respondent group consisted of “Professional Engineers/Technical Consultants” (54%), followed by “Researchers” (23%) and “Students” (15%) (see Figure 1). By domain, 44% of respondents worked in commercial aerospace or drone businesses, followed by “Academia and Education” (18%) and “Defense/Government” (16%).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: (Left) Pie-chart showing the primary professions of the survey’s participants and (right) pie-chart with the distribution of domains of the survey participants."
  },
  {
    "objectID": "docs/results.html#experience",
    "href": "docs/results.html#experience",
    "title": "",
    "section": "1.2 Experience",
    "text": "1.2 Experience\nIn terms of PX4 autopilot experience, the largest group of participants had 1-3 years of experience (33%), followed by those with 1 month to 1 year (20%), and those with over 5 years (17%). Regarding simulator expertise, most respondents rated themselves as intermediate users (39%), followed by advanced users (34%), with novices comprising the third-largest group (17%). Please see Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: (Left) Pie-chart showing the PX4 experience of the participants and (right) pie-chart showing the robot simulation experience."
  },
  {
    "objectID": "docs/results.html#profile-of-personas",
    "href": "docs/results.html#profile-of-personas",
    "title": "",
    "section": "1.3 Profile of Personas",
    "text": "1.3 Profile of Personas\nBased on these responses, we identified five distinct user personas depicted in Figure 3: “Senior Engineers (P1),” “Junior Engineers (P2),” “Academic Researchers (P3),” “Academic Students (P4),” and “Independent Hobbyists (P5).” These personas will be used to analyze and interpret the survey results in subsequent sections.\n\n\n\n\n\n\nFigure 3: Pie-chart with the types of personas selected from the profiles.\n\n\n\n\n1.3.1 P1 - Senior Engineers\nHighly experienced professional engineers with advanced to expert-level simulation expertise. These are the power users who deploy PX4 simulators in production environments for commercial products, defense systems, and critical applications.\n\nProfession: Professional Engineer / Technical Consultant\nExpertise Level: Advanced: 27 (75%) / Expert: 9 (25%)\nPrimary Domains: Aerospace/Drones (Commercial): 24 (67%), Defense/Government: 10 (28%), Research Institutions: 1 (3%), Other: 1 (2%)\n\n\n\n1.3.2 P2 - Junior Engineers\nProfessional engineers in commercial or defense settings who are building their drone and simulation expertise. This includes career switchers, new hires at drone companies, and engineers expanding their skillset to include simulation tools.\n\nProfession: Professional Engineer / Technical Consultant\nExpertise Level: Intermediate: 20 (69%), Novice: 9 (31%)\nPrimary Domains: Aerospace/Drones (Commercial): 18 (62%), Defense/Government: 6 (21%), Research Institutions: 2 (7%), Other: 3 (10%)\n\n\n\n1.3.3 P3 - Academic Researchers\nAcademic and industry researchers using PX4 simulators for cutting-edge research, publications, and experimental work. This group spans university researchers, PhD students/postdocs, and R&D engineers at companies and research institutions.\n\nProfession: Researcher (Academic/Industry)\nExpertise Level: Intermediate: 14 (50%), Advanced: 11 (39%), Expert: 2 (7%), Novice: 1 (4%)\nPrimary Domains: Academia/Education: 10 (36%), Aerospace/Drones (Commercial): 7 (25%), Research Institutions: 7 (25%), Defense/Government: 3 (11%), Aerospace/Drones (Hobbyist): 1 (3%)\n\n\n\n1.3.4 P4 - Academic Students\nUniversity students (BSc/MSc level) learning drone simulation as part of their coursework, research projects, or thesis work. This is the entry-level academic segment that represents the future pipeline of PX4 developers and users.\n\nProfession: Student (BSc/MSc)\nExpertise Level: Intermediate: 7 (39%), Novice: 8 (44%), Advanced: 2 (11%), Expert: 1 (6%)\nPrimary Domains: Academia/Education: 12 (67%), Aerospace/Drones (Hobbyist): 3 (17%), Aerospace/Drones (Commercial): 2 (11%), Other: 1 (5%)\n\n\n\n1.3.5 P5 - Independent Hobbyists\nDrone enthusiasts, hobbyists, and independent developers exploring PX4 for personal projects, skill development, or potential commercial opportunities. This small but passionate group contributes to community growth and grassroots adoption.\n\nProfession: Hobbyist / Independent Developer / Non-professional\nExpertise Level: Intermediate: 5 (63%), Novice: 2 (25%), Advanced: 1 (12%)\nPrimary Domains: Aerospace/Drones (Hobbyist): 6 (75%), Aerospace/Drones (Commercial): 1 (12%), Robotics: 1 (12%)"
  },
  {
    "objectID": "docs/results.html#participants-experience-with-simulation",
    "href": "docs/results.html#participants-experience-with-simulation",
    "title": "",
    "section": "1.4 Participants Experience with Simulation",
    "text": "1.4 Participants Experience with Simulation\nThis section presents results on participants’ past simulation experience and currently used simulators, as well as data on simulated vehicle types and hardware specifications.\n\n\n\n\n\n\nFigure 4: Bar graph with simulators experienced vs last used simulators. Light red stands for an unclear amount of current users using Gazebo New (previous ignition) due to ambiguity in the answers.\n\n\n\n\n1.4.1 Simulation Past Experience\nParticipants listed all simulators they had previously used, as shown in Figure 4. The two most common were Gazebo (New) with 94 responses and Gazebo Classic with 79 responses, the two officially supported simulators for PX4 autopilot. Usage drops significantly after these, with Simulink (29), Isaac Sim (28), and AirSim (26) following. The “Other” category included X-Plane, Webots, CoppeliaSim, SIH, and several custom-built simulators.\n\n\n1.4.2 Simulation Current Use\nFigure 4 also shows current simulator usage based on an open-ended question. Due to ambiguity about which version of Gazebo participants were using, the following post-processing classifications were applied:\n\nParticipants with experience only in Gazebo Classic were classified as currently using Gazebo Classic\nParticipants with experience only in Gazebo New were classified as currently using Gazebo New\nParticipants who specified “gz sim,” version names (Harmonic/Ionic), or clarified in comments were classified as Gazebo New\n\nAfter this classification, 30 participants who had used both versions could not be definitively categorized. These are shown in light red in Figure 4, with the assumption they likely use the most current and recommended simulator (Gazebo New), though this metric should be interpreted with caution. Overall, Gazebo New remains the most widely used simulator (46 confirmed users), followed by Gazebo Classic (19). AirSim and its forks account for 5 current users, while Simulink and JSBSim each have 4 users.\n\n\n1.4.3 Summary of Comments on Gazebo New\nSince Gazebo New is the most widely used simulator, the following summarizes user feedback on its strengths and weaknesses. Users appreciate the versatility of Gazebo New and its seamless integration with both PX4 SITL and ROS 2. They value the wide variety of available sensors—a feature lacking in many other simulators—as well as its open-source nature and large community support. Users also noted significant improvements in the physics model compared to Gazebo Classic.\nHowever, users identified several areas needing improvement. Multiple participants reported instability on operating systems other than Ubuntu, particularly macOS. Many prefer Gazebo Classic’s GUI, describing it as less complex to work with. Adding custom sensors through the plugin framework and creating custom vehicle models pose significant challenges for many developers, particularly due to inadequate documentation. Beginners especially struggle to find appropriate documentation.\nAcademic researchers (P3) specifically noted that Gazebo is not well-suited for reinforcement learning applications, leading them to use alternative simulators for this purpose. They also requested better multi-vehicle simulation support, such as the ability to simplify or disable physics for certain vehicles. Additionally, while some users praised the updated physics, others reported that flight dynamics and environmental variables still fall short of requirements.\n\n\n1.4.4 Simulated Vehicles\nParticipants indicated which vehicle types and payloads they typically simulate (Figure 5). Quadcopters are the most commonly simulated vehicle type (104), followed by multicopters (52), with fixed-wing and VTOL vehicles tied for third (50 each). For payloads, cameras are most prevalent (103), followed by range sensors such as LiDAR/ToF sensors (88), with gimbals and communication equipment sharing third place.\nRegarding the number of vehicles simulated simultaneously, the majority simulate only one vehicle (63%), followed by those who typically simulate 5-10 vehicles (26%), and 7% who simulate teams of 2-4 vehicles (see Figure 6).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: (Left) Bar graph with the different types simulated vehicles and (right) bar graph with the distribution of type of payloads simulated.\n\n\n\n\n\n\n\n\n\nFigure 6: Pie-chart of the amount of simulated vehicles that participants usually simulate.\n\n\n\n\n\n1.4.5 Simulation Host Specifications\nThe survey also asked questions about the host computers running the simulation, including GPU-enabled systems, OS distribution, and whether the simulation runs locally or remotely. Figure 7 shows the hardware specifications participants use to run their simulators. The majority use PCs with dedicated GPUs, with Ubuntu as the dominant operating system. Most participants run simulators locally on their machines rather than on remote servers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Pie-charts with the (left) GPU-enabled machines, the OS distribution (middle) and running location of the simulation (right)."
  },
  {
    "objectID": "docs/results.html#purpose-and-goals",
    "href": "docs/results.html#purpose-and-goals",
    "title": "",
    "section": "1.5 Purpose and Goals",
    "text": "1.5 Purpose and Goals\n\n1.5.1 Purpose for Using Simulators\nFigure 8 displays the reasons and purposes for using simulators. Participants most commonly use simulators for testing before real flights (89), followed by algorithm development (84) and robot prototyping and design (59).\nThe radar graph on the right of Figure 8 shows that both academic researchers and students also emphasize academic research and education/learning. Senior engineers (P1) strongly prioritize commercial product development, followed by junior engineers (P2) and independent developers/hobbyists (P5). Students (P4) and hobbyists (P5) also occasionally use simulation for competition preparation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: (Left) The bar graph with the count of indicated purposes of using simulators for all participants and (right) the radar graph with the purposes plotted per persona.\n\n\n\n\n\n1.5.2 Goals of Using Simulators\nThe bar graph on the left of Figure 9 shows that all participants use simulators as development tools for their robotic systems, followed by mission planning and validation (74) and quality assurance in continuous integration (48). The radar graph on the right reveals that senior engineers (P1) drive the high CI/QA scores, while academic researchers (P3) and independent hobbyists (P5) also use simulators for generating machine learning training data. A portion of students (P4) and hobbyists (P5) additionally indicated using simulation for safety testing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: (Left) A bar graph of the goals of using simulators for all participants and (right) the radar graph with goals plotted per persona."
  },
  {
    "objectID": "docs/results.html#features-and-priorities",
    "href": "docs/results.html#features-and-priorities",
    "title": "",
    "section": "1.6 Features and Priorities",
    "text": "1.6 Features and Priorities\nParticipants rated the perceived importance of simulator features on a scale from 1 (least important) to 5 (very important) across four categories: technical capabilities, platform & compatibility, usability and accessibility, and advanced features. These ratings are also analyzed by persona, as defined in the previous section.\n\n1.6.1 Technical Capabilities\nTable 1 presents the importance ratings for technical capabilities. Software-in-the-loop (SITL) is considered most important with a score of 4.63, followed by realistic physics simulation (4.38) and hardware-in-the-loop (3.82). Junior engineers (P2) showed a slightly different preference profile, rating failure mode simulation (3.96) and simulated networking capabilities (4.00) higher than other technical capabilities beyond SITL.\n\n\n\nTable 1: The average perceived importance of technical capabilities of simulators per profile and for all participants overall. The scale goes from 0 (not important) to 5 (very important).\n\n\n\n\n\n\n\n\n1.6.2 Platform and Compatibility\nTable 2 shows that the highest-rated platform and compatibility feature is ROS/ROS2 integration (3.92), followed by network capabilities and version compatibility between different PX4 releases (both 3.82). Researchers (P3) prioritized this category more than any other persona profiles. Notable deviations include senior engineers (P1), who rated Docker/containerization support (3.81) higher than network capabilities, and independent developers/hobbyists (P5), who rated network capabilities significantly higher than all other platform and compatibility features.\n\n\n\nTable 2: The average perceived importance of platform and compatibility of simulators per profile and for all participants overall.\n\n\n\n\n\n\n\n\n1.6.3 Usability and Accessibility\nTable 3 presents importance ratings for usability and accessibility features. Participants rated template and example library availability as most important (4.14), followed by real-time debugging support and easy installation process. Academic students (P4) prioritized this category more than any other persona. Academic researchers (P3), students (P4), and independent developers/hobbyists (P5) all rated real-time parameter tuning higher than easy installation process, deviating from the overall ranking.\n\n\n\nTable 3: The average perceived importance of usability and accessibility of simulators per profile and for all participants overall.\n\n\n\n\n\n\n\n\n1.6.4 Advanced Features\nTable 4 lists the importance ratings for advanced simulation features. Across all participants, custom sensor model creation was rated most important (4.17), followed by easy robot model creation (4.13) and scenario scripting and automation (3.94). Academic researchers (P3) prioritized this category more than any other persona. Ratings were relatively consistent across personas, with the notable exception that independent developers/hobbyists (P5) also rated swarm simulation as highly important.\n\n\n\nTable 4: The average perceived importance of advanced features of simulation per profile and for all participants overall.\n\n\n\n\n\n\n\n\n1.6.5 Top Wanted Functionalities\nAcross all personas, the top 5 most important features are:\n\nSoftware-in-the-loop support\nRealistic physics simulation\nCustom sensor model creation\nTemplate/example library availability\nEasy robot model creation\n\nTable 5 breaks down the top 5 features by persona. Software-in-the-loop capabilities are considered essential by all personas except students (P4), who prioritize template and example library availability and real-time parameter tuning more highly. Realistic physics simulation is universally valued, though independent developers/hobbyists (P5) rated network capabilities and template availability higher. ROS/ROS2 integration is more important to advanced simulator users (senior engineers (P1) and academic researchers (P3)), while both junior (P2) and senior engineers (P1) rated custom sensor creation and easy robot model creation as equally important.\n\n\n\nTable 5: Top 5 wanted features per profile."
  },
  {
    "objectID": "docs/results.html#final-choice-for-priority-roadmap",
    "href": "docs/results.html#final-choice-for-priority-roadmap",
    "title": "",
    "section": "1.7 Final Choice for Priority Roadmap",
    "text": "1.7 Final Choice for Priority Roadmap\nIn a final single-choice question, participants selected the one feature they would prioritize if only one item could be on the roadmap (see Figure 10). The largest group (18%) requested improvements to the current Gazebo integration with PX4 autopilot. Another 17% prioritized better documentation and examples for existing simulators, while 16% wanted more realistic physics and higher-quality sensor models.\n\n\n\n\n\n\nFigure 10: Pie-chart with the distribution of the final priority question of all participants.\n\n\n\nFigure 11 presents a bar chart showing how each persona influenced the roadmap priorities. Professional engineers, both senior (P1) and junior (P2), voted for improved Gazebo integration with PX4. Academic researchers (P3) prioritized more realistic sensor and physics models along with better multi-vehicle/swarm simulation support. A significant portion of senior engineers (P1) also voted for reducing the sim-to-real gap, while some advocated for a new PX4-optimized simulator. Junior engineers (P2) emphasized enhanced simulation creation capabilities. Both junior engineers (P2) and academic students (P4), representing novice and intermediate users, indicated they would benefit most from better documentation and examples for existing PX4 simulation integrations. Finally, both senior engineers (P1) and academic students (P4) requested improved ROS/ROS2 integration with PX4 simulation.\n\n\n\n\n\n\nFigure 11: Bar chart to indicate the differences per persona on the final roadmap priority choice of the survey."
  },
  {
    "objectID": "docs/survey_content_pilot.html",
    "href": "docs/survey_content_pilot.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n1 A. PX4 Simulator Community Survey - Pilot\nThis survey aims to understand the developer needs of PX4 users and their simulation experiences. We’re hoping to improve the current situation and make things better for everyone.\nThis survey should take only 10-15 minutes to fill in.\n\n\nGetting to Know You\n\nWe’d like to understand your background and experience level to better contextualize your feedback.\n1. What is your primary profession?\n\nStudent (BSc/MSc)\nResearcher (Academic/Industry)\nEducator/Teacher\nProfessional Engineer / Technical Consultant\nOther: ___________\n\n2. What is your level of expertise regarding robotic simulators?\n\nNovice\nIntermediate\nAdvanced\nExpert\n\n3. How long have you been working with PX4?\n\nI haven’t worked with PX4\nI just started working with PX4 (&lt; 1 month)\n1 month - 1 year\n1-3 years\n3-5 years\n5+ years\n\n\n\nCurrent Simulator Usage\n\nThis section helps us understand which tools you’re using and how you’re using them in your current workflow.\n4. Which simulators have you worked with in your setup? (Select all that apply)\n\nNvidia Isaac Sim\nGazebo (formerly known as Ignition)\nGazebo Classic\nAirSim\nMuJoCo\nPybullet\nFlightGear\nJSBSim\nOther: ___________\n\n5. What was your primary purpose for using simulators? (Select all that apply)\n\nAcademic research\nEducation/learning\nRobot prototyping/design\nAlgorithm development\nTesting before real flights\nDemonstration/presentation\nCompetition preparation\nCommercial product development\nFlight tooling design\nOther: ___________\n\n6. For which goals are you using simulators? (Select all that apply)\n\nDevelopment tool for robotic systems\nQuality assurance in continuous integration\nData generation for machine learning\nMission planning and validation\nTraining pilots/operators\nSafety\nRegulatory compliance testing\nOther: ___________\n\n7. What type of aerial robotic vehicle are you working with?\n\nHelicopter\nQuadcopter\nMulticopters (more than 4 propellers)\nFixed wing\nHybrid / VTOL vehicles\nBlimps\nOther: ___________\n\n8. Does your platform have additional equipment or payloads? (Select all that apply)\n\nManipulator arm\nGripper/claw\nGimbal\nCamera\nCargo hook/winch\nPerception Sensors (lidar, sonar, etc.)\nCommunication equipment\nNone of the above\nOther: ___________\n\n9. How many robots/UAVs do you typically simulate simultaneously?\n\nJust one (1)\n2-4\n5-10\n10-100\n100+\n\n\n\nTechnical Setup & Constraints\n\nUnderstanding your hardware setup and limitations helps us design tools that work for everyone’s environment.\n10. Did you have to purchase/upgrade your computer specifically for simulation?\n\nYes, bought entirely new system\nYes, upgraded existing system\nNo, used existing hardware\nUsing cloud/remote computing\nOther: ___________\n\n11. What operating system do you primarily use for simulation?\n\nUbuntu\nArch Linux\nFedora Linux\nWindows\nMacOS\nOther: ___________\n\n12. Which version of the operating system have you selected above? ___________\n13. Do you use a GPU enabled computer, and if so, what kind of specifications does it have? ___________\n14. What’s your biggest hardware limitation?\n\nCPU performance\nGPU/graphics performance\nRAM capacity\nStorage space\nNetwork bandwidth\nNo significant limitations\nOther: ___________\n\n15. Do you find the necessary hardware/computer specifications for simulation acceptable or workable?\n\nYes\nNo\n\n16. Anything to add to any of the questions answered in this section regarding computer specifications? ___________\n\n\nCurrent Simulator Experience\n\nHere we dive deep into your experience with simulators - what works well and what doesn’t. This feedback is crucial for identifying the biggest areas for improvement.\nPlease fill in the survey based on your first instinct (don’t linger too long per question)\n17. Which simulator have you used most recently? ___________\n18. Setup and Installation Experience\nRate on scale: 1 (I agree) to 5 (I do not agree)\n\n“The simulator was easy to install.”\n“I had to install too many external dependencies”\n“The error and debug messages were easy to understand”\n\n19. User Experience\nRate on scale: 1 (I agree) to 5 (I do not agree)\n\n“I needed to use an extensive guide to navigate through the GUI”\n“Using the simulator was a smooth experience.”\n“The user interface felt cluttered and complex.”\n“The error/bug messages were easy to read”\n\n20. Technical Performance\nRate on scale: 1 (I agree) to 5 (I do not agree)\n\n“Objects in the simulation didn’t behave realistically.”\n“The simulator can run simulations faster than real-time.”\n“The simulation’s graphics were very photo-realistic.”\n“I experienced frequent crashes and freezes.”\n\n21. Feature Support\nRate on scale: 1 (I agree) to 5 (I do not agree)\n\n“The simulator offered a wide variety of sensors.”\n“I can not easily make a custom drone in this simulator”\n“I’m able to use this simulator easily with swarms”\n\n22. Documentation and Tutorials\nRate on scale: 1 (I agree) to 5 (I do not agree)\n\n“The documentation/website was easy to find.”\n“The documentation was difficult to follow.”\n“There were plenty of tutorials available.”\n\n23. Community and Support\nRate on scale: 1 (I agree) to 5 (I do not agree)\n\n“It was hard to find a clear way to get help or ask questions.”\n“The community and support forums were helpful.”\n“It took too long to get my questions answered.”\n\n24. What do you like the most about current/latest simulator you are using? ___________\n25. What’s your biggest frustration with current/latest simulator you are using? ___________\n26. Anything else to add? ___________\n\n\nDesired Features & Priorities\n\nThis section helps us prioritize development efforts by understanding which features matter most to different types of users.\nPlease fill in the survey based on your first instinct (don’t linger too long per question)\n27. Please rate the preferred Technical Capabilities\nRate on scale: 1 (Not important) to 5 (Very important), or No preference/opinion\n\nHardware-in-the-loop support\nSoftware-in-the-loop support\nFaster than real-time simulation\nPhotorealistic rendering\nRealistic physics simulation\nWeather/environmental simulation\nFailure mode simulation\nCommunication simulation\n\n28. Please rate the preferred Platform & Compatibility\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nMultiple OS support\nROS/ROS 2 integration\nCloud simulation capabilities\nDocker/containerization support\n\n29. Please rate the preferred Usability & Accessibility\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nBeginner-friendly interface\nEasy installation process\nDrag-and-drop model capability\nTemplate/example library availability\nReal-time parameter tuning\n\n30. Please rate the preferred Advanced Features\nRate on scale: 1 (Not important) to 5 (Very important), or No opinion or preference\n\nMulti-agent/swarm simulation\nMachine learning integration\nCustom sensor model creation\nEasy robot model creation\nScenario scripting/automation\nPerformance benchmarking tools\nCAD program integration for custom models\n\n31. Any more features that we should pay attention too as well? ___________\n\n\nFinal Questions\n\nFinal thoughts and opportunities for deeper engagement with our improvement efforts.\n32. What’s the most important improvement needed in simulation supported in the PX4-ecosystem? ___________\n33. Any additional comments or suggestions? ___________\n34. Would you be willing to participate in follow-up interviews? - Yes - No\n35. If yes, please provide on which email address we can contact you. Your email address will only be used for this exact purpose. ___________\n\n\nThank you!\n\nThank you for taking the time to share your experiences! Your feedback will help improve simulation tools for the entire PX4 community.\nPlease press next to submit the form"
  }
]